# Ollama URL for the backend to connect
# The path '/ollama' will be redirected to the specified backend URL
OLLAMA_BASE_URL='http://localhost:11434'

# Name of the compAInion instance
COMPAINION_NAME='Kommunal-GPT'

# ==============================
# Docker image configuration
# ==============================
# Ollama image + tag
OLLAMA_IMAGE='ollama/ollama'
OLLAMA_DOCKER_TAG='latest'

# Open WebUI image + tag
WEBUI_IMAGE='ghcr.io/open-webui/open-webui'
WEBUI_TAG='latest'

# Apache Tika image + tag
TIKA_IMAGE='apache/tika'
TIKA_TAG='latest-full'

# ==============================
# Ports
# ==============================
WEBUI_PORT=3000
TIKA_PORT=9998

# ==============================
# Open WebUI feature flags
# ==============================
ENABLE_SIGNUP=false
DEFAULT_LOCALE=de
USE_CUDA_DOCKER=true
ENABLE_FOLLOW_UP_GENERATION=false
